{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"RECIST vs Reality Authors: Caryn Geady , Katy Scott Contact: bhklab.caryngeady@gmail.com Description: Code for RECIST to Reality manuscript. Set Up Prerequisites Pixi is required to run this project. If you haven't installed it yet, follow these instructions Installation Clone this repository to your local machine Navigate to the project directory Set up the environment using Pixi: pixi install Documentation Click here to view the full documentation.","title":"Home"},{"location":"#recist-vs-reality","text":"Authors: Caryn Geady , Katy Scott Contact: bhklab.caryngeady@gmail.com Description: Code for RECIST to Reality manuscript.","title":"RECIST vs Reality"},{"location":"#set-up","text":"","title":"Set Up"},{"location":"#prerequisites","text":"Pixi is required to run this project. If you haven't installed it yet, follow these instructions","title":"Prerequisites"},{"location":"#installation","text":"Clone this repository to your local machine Navigate to the project directory Set up the environment using Pixi: pixi install","title":"Installation"},{"location":"#documentation","text":"Click here to view the full documentation.","title":"Documentation"},{"location":"data_sources/","text":"Data Sources External Data Sources CPTAC-CCRCC CPTAC-HNSCC CPTAC-PDA CPTAC-UCEC NSCLC-Radiogenomics Crowds-Cure-2018 CPTAC-CCRCC CPTAC-HNSCC Name : The Clinical Proteomic Tumor Analysis Consortium Head and Neck Squamous Cell Carcinoma Collection Version/Date : Version 14 or 15? URL : https://www.cancerimagingarchive.net/collection/cptac-hnscc/ Access Method : NBIA Data Retriever Access Date : 2024-03-29 Data Format : DICOM-CT, DICOM-RTSTRUCT Citation : National Cancer Institute Clinical Proteomic Tumor Analysis Consortium (CPTAC). (2018). The Clinical Proteomic Tumor Analysis Consortium Head and Neck Squamous Cell Carcinoma Collection (CPTAC-HNSCC) (Version 19) [dataset]. The Cancer Imaging Archive. https://doi.org/10.7937/k9/tcia.2018.uw45nh81 License : TCIA Restricted License Data Types : Images: CT, RTSTRUCT Sample Size Selected : TBD PatientIDs : C3N-00297 C3N-00498 C3N-00828 C3N-01620 C3N-01752 C3N-01754 C3N-01757 C3N-01943 C3N-01948 CPTAC-PDA CPTAC-UCEC NSCLC-Radiogenomics Crowds-Cure-2018 Name : Crowds Cure Cancer: Data collected at the RSNA 2018 annual meeting Version/Date : Version 1: Updated 2019/05/30 URL : https://www.cancerimagingarchive.net/analysis-result/crowds-cure-2018/ Access Method : NBIA Data Retriever Access Date : 2025-03-26 Data Format : DICOM-SR Citation : Urban, T., Ziegler, E., Pieper, S., Kirby, J., Rukas, D., Beardmore, B., Somarouthu, B., Ozkan, E., Lelis, G., Fevrier-Sullivan, B., Nandekar, S., Beers, A., Jaffe, C., Freymann, J., Clunie, D., Harris, G. J., & Kalpathy-Cramer, J. (2019). Crowds Cure Cancer: Crowdsourced data collected at the RSNA 2018 annual meeting [Data set]. The Cancer Imaging Archive. https://doi.org/10.7937/TCIA.2019.yk0gm1eb License : CC BY 3.0 Data Types : Annotations: Structured Reports containing 2D RECIST measurements Sample Size Selected : TBD Overview This section should document all data sources used in your project. Proper documentation ensures reproducibility and helps others understand your research methodology. How to Document Your Data For each data source, include the following information: 1. External Data Sources Name : Official name of the dataset Version/Date : Version number or access date URL : Link to the data source Access Method : How the data was obtained (direct download, API, etc.) Access Date : When the data was accessed/retrieved Data Format : Format of the data (FASTQ, DICOM, CSV, etc.) Citation : Proper academic citation if applicable License : Usage restrictions and attribution requirements Example: ## TCGA RNA-Seq Data - **Name**: The Cancer Genome Atlas RNA-Seq Data - **Version**: Data release 28.0 - March 2021 - **URL**: https://portal.gdc.cancer.gov/ - **Access Method**: GDC Data Transfer Tool - **Access Date**: 2021-03-15 - **Citation**: The Cancer Genome Atlas Network. (2012). Comprehensive molecular portraits of human breast tumours. Nature, 490(7418), 61-70. - **License**: [NIH Genomic Data Sharing Policy](https://sharing.nih.gov/genomic-data-sharing-policy) 2. Internal/Generated Data Name : Descriptive name of the dataset Creation Date : When the data was generated Creation Method : Brief description of how the data was created Input Data : What source data was used Processing Scripts : References to scripts/Github Repo used to generate this data Example: ## Processed RNA-Seq Data - **Name**: Processed RNA-Seq Data for TCGA-BRCA - **Creation Date**: 2021-04-01 - **Creation Method**: Processed using kallisto and DESeq2 - **Input Data**: FASTQ Data obtained from the SRA database - **Processing Scripts**: [GitHub Repo](https://github.com/tcga-brca-rnaseq) 3. Data Dictionary For complex datasets, include a data dictionary that explains: Column Name Data Type Description Units Possible Values patient_id string Unique patient identifier N/A TCGA-XX-XXXX format age integer Patient age at diagnosis years 18-100 expression float Gene expression value TPM Any positive value Best Practices Store raw data in data/rawdata/ and never modify it Store processed data in data/procdata/ and all code used to generate it should be in workflow/scripts/ Document all processing steps Track data provenance (where data came from and how it was modified) Respect data usage agreements and licenses! This is especially important for data that should not be shared publicly","title":"Data Sources"},{"location":"data_sources/#data-sources","text":"","title":"Data Sources"},{"location":"data_sources/#external-data-sources","text":"CPTAC-CCRCC CPTAC-HNSCC CPTAC-PDA CPTAC-UCEC NSCLC-Radiogenomics Crowds-Cure-2018","title":"External Data Sources"},{"location":"data_sources/#cptac-ccrcc","text":"","title":"CPTAC-CCRCC"},{"location":"data_sources/#cptac-hnscc","text":"Name : The Clinical Proteomic Tumor Analysis Consortium Head and Neck Squamous Cell Carcinoma Collection Version/Date : Version 14 or 15? URL : https://www.cancerimagingarchive.net/collection/cptac-hnscc/ Access Method : NBIA Data Retriever Access Date : 2024-03-29 Data Format : DICOM-CT, DICOM-RTSTRUCT Citation : National Cancer Institute Clinical Proteomic Tumor Analysis Consortium (CPTAC). (2018). The Clinical Proteomic Tumor Analysis Consortium Head and Neck Squamous Cell Carcinoma Collection (CPTAC-HNSCC) (Version 19) [dataset]. The Cancer Imaging Archive. https://doi.org/10.7937/k9/tcia.2018.uw45nh81 License : TCIA Restricted License Data Types : Images: CT, RTSTRUCT Sample Size Selected : TBD PatientIDs : C3N-00297 C3N-00498 C3N-00828 C3N-01620 C3N-01752 C3N-01754 C3N-01757 C3N-01943 C3N-01948","title":"CPTAC-HNSCC"},{"location":"data_sources/#cptac-pda","text":"","title":"CPTAC-PDA"},{"location":"data_sources/#cptac-ucec","text":"","title":"CPTAC-UCEC"},{"location":"data_sources/#nsclc-radiogenomics","text":"","title":"NSCLC-Radiogenomics"},{"location":"data_sources/#crowds-cure-2018","text":"Name : Crowds Cure Cancer: Data collected at the RSNA 2018 annual meeting Version/Date : Version 1: Updated 2019/05/30 URL : https://www.cancerimagingarchive.net/analysis-result/crowds-cure-2018/ Access Method : NBIA Data Retriever Access Date : 2025-03-26 Data Format : DICOM-SR Citation : Urban, T., Ziegler, E., Pieper, S., Kirby, J., Rukas, D., Beardmore, B., Somarouthu, B., Ozkan, E., Lelis, G., Fevrier-Sullivan, B., Nandekar, S., Beers, A., Jaffe, C., Freymann, J., Clunie, D., Harris, G. J., & Kalpathy-Cramer, J. (2019). Crowds Cure Cancer: Crowdsourced data collected at the RSNA 2018 annual meeting [Data set]. The Cancer Imaging Archive. https://doi.org/10.7937/TCIA.2019.yk0gm1eb License : CC BY 3.0 Data Types : Annotations: Structured Reports containing 2D RECIST measurements Sample Size Selected : TBD","title":"Crowds-Cure-2018"},{"location":"data_sources/#overview","text":"This section should document all data sources used in your project. Proper documentation ensures reproducibility and helps others understand your research methodology.","title":"Overview"},{"location":"data_sources/#how-to-document-your-data","text":"For each data source, include the following information:","title":"How to Document Your Data"},{"location":"data_sources/#1-external-data-sources","text":"Name : Official name of the dataset Version/Date : Version number or access date URL : Link to the data source Access Method : How the data was obtained (direct download, API, etc.) Access Date : When the data was accessed/retrieved Data Format : Format of the data (FASTQ, DICOM, CSV, etc.) Citation : Proper academic citation if applicable License : Usage restrictions and attribution requirements Example: ## TCGA RNA-Seq Data - **Name**: The Cancer Genome Atlas RNA-Seq Data - **Version**: Data release 28.0 - March 2021 - **URL**: https://portal.gdc.cancer.gov/ - **Access Method**: GDC Data Transfer Tool - **Access Date**: 2021-03-15 - **Citation**: The Cancer Genome Atlas Network. (2012). Comprehensive molecular portraits of human breast tumours. Nature, 490(7418), 61-70. - **License**: [NIH Genomic Data Sharing Policy](https://sharing.nih.gov/genomic-data-sharing-policy)","title":"1. External Data Sources"},{"location":"data_sources/#2-internalgenerated-data","text":"Name : Descriptive name of the dataset Creation Date : When the data was generated Creation Method : Brief description of how the data was created Input Data : What source data was used Processing Scripts : References to scripts/Github Repo used to generate this data Example: ## Processed RNA-Seq Data - **Name**: Processed RNA-Seq Data for TCGA-BRCA - **Creation Date**: 2021-04-01 - **Creation Method**: Processed using kallisto and DESeq2 - **Input Data**: FASTQ Data obtained from the SRA database - **Processing Scripts**: [GitHub Repo](https://github.com/tcga-brca-rnaseq)","title":"2. Internal/Generated Data"},{"location":"data_sources/#3-data-dictionary","text":"For complex datasets, include a data dictionary that explains: Column Name Data Type Description Units Possible Values patient_id string Unique patient identifier N/A TCGA-XX-XXXX format age integer Patient age at diagnosis years 18-100 expression float Gene expression value TPM Any positive value","title":"3. Data Dictionary"},{"location":"data_sources/#best-practices","text":"Store raw data in data/rawdata/ and never modify it Store processed data in data/procdata/ and all code used to generate it should be in workflow/scripts/ Document all processing steps Track data provenance (where data came from and how it was modified) Respect data usage agreements and licenses! This is especially important for data that should not be shared publicly","title":"Best Practices"},{"location":"usage/","text":"Usage Guide Project Configuration Each dataset needs a configuration YAML file with the following settings filled in DATA_SOURCE: \"\" # where the data came from, will be used for data organization DATASET_NAME: \"\" # the name of the dataset , will be use for data organization ### MED-IMAGETOOLS settings MIT: MODALITIES: # Modalities to process with autopipeline image: CT mask: RTSTRUCT ROI_STRATEGY: MERGE # How to handle multiple ROI matches ROI_MATCH_MAP: # Matching map for ROIs in dataset (use if you only want to process some of the masks in a segmentation) KEY:ROI_NAME # NOTE: there can be no spaces in KEY:ROI_NAME The file should be saved in the config directory and named {DATASET_NAME}.yaml . Data Setup The following sections describe how to set up the data you wish to process with this pipeline following the BHKLab Data Management Protocol (DMP). This will ensure data remains separate from the project directory and accessible to other users. Raw Data Set up a separate main data directory outside of the project directory. We'll call this Datasets . In Datasets , set up a directory for the dataset you wish to process as follows: Datasets |---- {DATASET_SOURCE}_{DATASET_NAME} |-- clinical | `-- {Clinical Data File}.csv OR {Clinical Data File}.xlsx `-- images |-- {DATASET_NAME} | |-- {PatientID} | | `-- {StudyUID} | | |-- {Image DICOM directory} | | | |-- 1-01.dcm | | | |-- ... | | | |-- 1-N.dcm | | |-- {Mask DICOM directory} | | | `-- 1-01.dcm | |-- {PatientID} | |-- ... | `-- {PatientID} `-- annotations `-- {DATASET_NAME} |-- DICOM-SR_annotation_file.dcm |-- DICOM-SR_annotation_file.dcm `-- DICOM-SR_annotation_file.dcm Image directory structure may vary depending on the source. This example is based on the structure setup by TCIA when downloading with a manifest file. However, for the pipeline to run correctly, images/{DATASET_NAME} must exist in the {DATASET_SOURCE}_{DATASET_NAME} directory. Everything within {DATASET_NAME} may vary though. !!! note \"BHKLab DMP Setup\" If using the BHKLab DMP, the Datasets directory will be structured with rawdata/{DiseaseRegion}/{DATASET_SOURCE}_{DATASET_NAME} . In the next step, you can create the symbolic link starting from {DATASET_SOURCE}_{DATASET_NAME} . Once this data directory is setup, run the following in a terminal from the main directory of the project. ln -s /path/to/Datasets/{DATASET_SOURCE}_{DATASET_NAME} data/rawdata This will create a symbolic link to your dataset in the Datasets directory. You can confirm this worked by running: ls -l data/rawdata and you should see, total 5 -rw-rw-r-- 1 bhkuser root 1395 Jun 4 15:21 README.md lrwxrwxrwx 1 bhkuser root 80 Jun 4 15:46 {DATASET_SOURCE}_{DATASET_NAME} -> /path/to/Datasets/{DATASET_SOURCE}_{DATASET_NAME} Now, document the dataset you've added on the Data Sources page following the provided template. Processed Data If you wish to use the BHKLab DMP strategy, follow the process below. Create a processed data directory for your dataset in the external Datasets directory as follows: mkdir /path/to/Datasets/procdata/{DiseaseRegion}/{DATASET_SOURCE}_{DATASET_NAME} Note : the Disease Region must match what is in the rawdata path to the dataset. Now you can create a symbolic link to this directory in the project directory, like we did for the raw data. ln -s /path/to/Datasets/procdata/{DiseaseRegion}/{DATASET_SOURCE}_{DATASET_NAME} data/procdata Results TODO:: describe results directory setup Running Your Analysis 1. Running Med-ImageTools The first step in the pipeline is to run Med-ImageTools index and autopipeline to organize and process the image and mask data. Requirements: 1. You've set up a dataset config yaml file as described above. 2. You've set up the symbolic links for the dataset in both rawdata and procdata From the home project directory, run the following: pixi run mit config/{DATASET_NAME}.yaml This will generate NiFTi files for each image and it's corresponding mask, where the masks will be named KEY__[ROI_NAME] . The output format should be as follows: ``bash data -- procdata -- {DATASET_SOURCE}_{DATASET_NAME} -- images -- mit_{DATASET_NAME} -- {PatientID} {SampleNumber} |-- {ImageModality} {SeriesInstanceUID} | -- {ImageModality}.nii.gz -- {SegmentationModality}_{SeriesInstanceUID} `-- {KEY}__[{ROI_name}].nii.gz","title":"Usage"},{"location":"usage/#usage-guide","text":"","title":"Usage Guide"},{"location":"usage/#project-configuration","text":"Each dataset needs a configuration YAML file with the following settings filled in DATA_SOURCE: \"\" # where the data came from, will be used for data organization DATASET_NAME: \"\" # the name of the dataset , will be use for data organization ### MED-IMAGETOOLS settings MIT: MODALITIES: # Modalities to process with autopipeline image: CT mask: RTSTRUCT ROI_STRATEGY: MERGE # How to handle multiple ROI matches ROI_MATCH_MAP: # Matching map for ROIs in dataset (use if you only want to process some of the masks in a segmentation) KEY:ROI_NAME # NOTE: there can be no spaces in KEY:ROI_NAME The file should be saved in the config directory and named {DATASET_NAME}.yaml .","title":"Project Configuration"},{"location":"usage/#data-setup","text":"The following sections describe how to set up the data you wish to process with this pipeline following the BHKLab Data Management Protocol (DMP). This will ensure data remains separate from the project directory and accessible to other users.","title":"Data Setup"},{"location":"usage/#raw-data","text":"Set up a separate main data directory outside of the project directory. We'll call this Datasets . In Datasets , set up a directory for the dataset you wish to process as follows: Datasets |---- {DATASET_SOURCE}_{DATASET_NAME} |-- clinical | `-- {Clinical Data File}.csv OR {Clinical Data File}.xlsx `-- images |-- {DATASET_NAME} | |-- {PatientID} | | `-- {StudyUID} | | |-- {Image DICOM directory} | | | |-- 1-01.dcm | | | |-- ... | | | |-- 1-N.dcm | | |-- {Mask DICOM directory} | | | `-- 1-01.dcm | |-- {PatientID} | |-- ... | `-- {PatientID} `-- annotations `-- {DATASET_NAME} |-- DICOM-SR_annotation_file.dcm |-- DICOM-SR_annotation_file.dcm `-- DICOM-SR_annotation_file.dcm Image directory structure may vary depending on the source. This example is based on the structure setup by TCIA when downloading with a manifest file. However, for the pipeline to run correctly, images/{DATASET_NAME} must exist in the {DATASET_SOURCE}_{DATASET_NAME} directory. Everything within {DATASET_NAME} may vary though. !!! note \"BHKLab DMP Setup\" If using the BHKLab DMP, the Datasets directory will be structured with rawdata/{DiseaseRegion}/{DATASET_SOURCE}_{DATASET_NAME} . In the next step, you can create the symbolic link starting from {DATASET_SOURCE}_{DATASET_NAME} . Once this data directory is setup, run the following in a terminal from the main directory of the project. ln -s /path/to/Datasets/{DATASET_SOURCE}_{DATASET_NAME} data/rawdata This will create a symbolic link to your dataset in the Datasets directory. You can confirm this worked by running: ls -l data/rawdata and you should see, total 5 -rw-rw-r-- 1 bhkuser root 1395 Jun 4 15:21 README.md lrwxrwxrwx 1 bhkuser root 80 Jun 4 15:46 {DATASET_SOURCE}_{DATASET_NAME} -> /path/to/Datasets/{DATASET_SOURCE}_{DATASET_NAME} Now, document the dataset you've added on the Data Sources page following the provided template.","title":"Raw Data"},{"location":"usage/#processed-data","text":"If you wish to use the BHKLab DMP strategy, follow the process below. Create a processed data directory for your dataset in the external Datasets directory as follows: mkdir /path/to/Datasets/procdata/{DiseaseRegion}/{DATASET_SOURCE}_{DATASET_NAME} Note : the Disease Region must match what is in the rawdata path to the dataset. Now you can create a symbolic link to this directory in the project directory, like we did for the raw data. ln -s /path/to/Datasets/procdata/{DiseaseRegion}/{DATASET_SOURCE}_{DATASET_NAME} data/procdata","title":"Processed Data"},{"location":"usage/#results","text":"TODO:: describe results directory setup","title":"Results"},{"location":"usage/#running-your-analysis","text":"","title":"Running Your Analysis"},{"location":"usage/#1-running-med-imagetools","text":"The first step in the pipeline is to run Med-ImageTools index and autopipeline to organize and process the image and mask data. Requirements: 1. You've set up a dataset config yaml file as described above. 2. You've set up the symbolic links for the dataset in both rawdata and procdata From the home project directory, run the following: pixi run mit config/{DATASET_NAME}.yaml This will generate NiFTi files for each image and it's corresponding mask, where the masks will be named KEY__[ROI_NAME] . The output format should be as follows: ``bash data -- procdata -- {DATASET_SOURCE}_{DATASET_NAME} -- images -- mit_{DATASET_NAME} -- {PatientID} {SampleNumber} |-- {ImageModality} {SeriesInstanceUID} | -- {ImageModality}.nii.gz -- {SegmentationModality}_{SeriesInstanceUID} `-- {KEY}__[{ROI_name}].nii.gz","title":"1. Running Med-ImageTools"},{"location":"devnotes/devnotes_caryn/","text":"Developer Notes - Caryn Purpose of This Section This section is for documenting technical decisions, challenges, and solutions encountered during your project. These notes are valuable for: Future you (who will forget why certain decisions were made) Collaborators who join the project later People coming from your publication who want to reproduce your work Anyone who might want to extend your research What to Document Design Decisions Document important decisions about your project's architecture, algorithms, or methodologies: ## Choice of RNA-Seq Analysis Pipeline [2025-04-25] We chose the kallisto over STAR pipeline for the following reasons: 1. The CCLE dataset is very large, and kallisto is faster for quantifying large datasets 2. GDSC used kallisto, so we can compare our results with theirs Technical Challenges Record significant problems you encountered and how you solved them ## Sample Name Format Issue [2025-04-25] We encountered a problem with sample name formats between the CCLE and GDSC datasets. The CCLE dataset uses \"BRCA-XX-XXXX\" format, while the GDSC dataset uses \"BRCA-XX-XXXX-XX\". We had to write a script to remove the last two characters from the sample names in the GDSC dataset. Dependencies and Environment Document specific version requirements or compatibility issues: ## Critical Version Dependencies [2025-04-25] SimpleITK 2.4.1 introduced a bug that flips images, so we froze version 2.4.0 Best Practices Date your entries when appropriate Link to relevant code files or external resources Include small code snippets when helpful Note alternatives you considered and why they were rejected Document failed approaches to prevent others from repeating mistakes Update notes when major changes are made to the approach","title":"Caryn"},{"location":"devnotes/devnotes_caryn/#developer-notes-caryn","text":"","title":"Developer Notes - Caryn"},{"location":"devnotes/devnotes_caryn/#purpose-of-this-section","text":"This section is for documenting technical decisions, challenges, and solutions encountered during your project. These notes are valuable for: Future you (who will forget why certain decisions were made) Collaborators who join the project later People coming from your publication who want to reproduce your work Anyone who might want to extend your research","title":"Purpose of This Section"},{"location":"devnotes/devnotes_caryn/#what-to-document","text":"","title":"What to Document"},{"location":"devnotes/devnotes_caryn/#design-decisions","text":"Document important decisions about your project's architecture, algorithms, or methodologies: ## Choice of RNA-Seq Analysis Pipeline [2025-04-25] We chose the kallisto over STAR pipeline for the following reasons: 1. The CCLE dataset is very large, and kallisto is faster for quantifying large datasets 2. GDSC used kallisto, so we can compare our results with theirs","title":"Design Decisions"},{"location":"devnotes/devnotes_caryn/#technical-challenges","text":"Record significant problems you encountered and how you solved them ## Sample Name Format Issue [2025-04-25] We encountered a problem with sample name formats between the CCLE and GDSC datasets. The CCLE dataset uses \"BRCA-XX-XXXX\" format, while the GDSC dataset uses \"BRCA-XX-XXXX-XX\". We had to write a script to remove the last two characters from the sample names in the GDSC dataset.","title":"Technical Challenges"},{"location":"devnotes/devnotes_caryn/#dependencies-and-environment","text":"Document specific version requirements or compatibility issues: ## Critical Version Dependencies [2025-04-25] SimpleITK 2.4.1 introduced a bug that flips images, so we froze version 2.4.0","title":"Dependencies and Environment"},{"location":"devnotes/devnotes_caryn/#best-practices","text":"Date your entries when appropriate Link to relevant code files or external resources Include small code snippets when helpful Note alternatives you considered and why they were rejected Document failed approaches to prevent others from repeating mistakes Update notes when major changes are made to the approach","title":"Best Practices"},{"location":"devnotes/devnotes_kaitlyn/","text":"Developer Notes - Kaitlyn Annotation-Imaging-Segmentation Matching Additional Function Documentation (Last Updated [2025-09-17]) get_ann_measurements() The DICOM data extracted through pydicom.dcmread is in a similar structure to nested dictionaries. Basic information (PatientID, annotation ID, etc.) can be found from the outer dictionary. The reference imaging information is found within the Current Requested Procedure Evidence Sequence one level down from the outer dictionary. The annotation information is found within nested content sequences, with the outermost content sequence (I've called the parent content sequence) usually containing 5 items with information about language, country of origin, etc., the file itself, the procedure(s) used, and measurements. Measurements are usually the last item in the parent sequence (at index 4). Each measurement is kept in another content sequence. The number of these content sequences are determined by the number of measurements taken. Must loop through all of them to get all annotation measurements. The content sequence each measurement is found in usually has a length of 5. The long axis measurement information is found in the 3rd item (index 2) and the short axis measurement information is found in the 4th item (index 3). Once inside either the long axis or short axis content sequence, you can get the following corresponding information: * Measurement Type: Found by searching the Concept Name Code Sequence for the first item and then obtaining the code meaning. The value of the measurement type should be \"Long Axis\" or \"Short Axis\" depending on what is being looked for. * Measurement Unit: Founed by searching the Measured Value Sequence for the first item, then searching for the Measurement Units Code Sequence first item, and then accessing the Code Value. This is usually \"mm\". * Annotation Measurement: Found by searching the Measured Value Sequence for the first item (same as above) and accessing the Numeric Value * Referenced SOPUID: Denotes the slice that the measurement was taken on (paired long and short axis measurements should be on the same slice). Found by entering the first item of two additional content sequences and the Referenced SOP Sequence and then accessing the Referenced SOP Instance UID value. This number should correspond to an instance value in the matching image (listed for each image in the crawl_db.json file). * Axis Coordinates: Stored as x1/y1/x2/y2 in the DICOM data, these are the points that define the annotation drawn. Found by searching the first item of one additional content sequence and looping through the values of Graphic Data, which should always have a length of 4. get_rtstruct_SOPUIDs(): This function is currently necessary due to an issue with med-imagetools not extracting the slice IDs associated with RTSTRUCT segmentation files specifially. This type of file will not have any ReferencedSOPUIDs listed in the crawl_db.json function, unlike the SEG files. These SOPUIDs are necessary to match the annotation with the segmentation. Similarly to get_ann_measurements() , the referenced slice IDs are found in a nested dictionary of sequences, each with only one item in them. The Referenced Frame of Reference Sequence contains information related to the slices in the segmentation file, the frame of reference, and the image the segmentation pertains to. The list of slices may only contain slices with actual segmentation or it could contain all slices within the referenced image (including where the binary mask is all 0 for a slice) . Once inside the Referenced Frame of Reference Sequence, the RT Referenced Study Sequence of length 1 will need to be entered. This includes all information in the previous sequence except for the frame of reference information. At this level, you can access the study instance UID and the RT Referenced Series Sequence with length 1. This contains the information related to the imaging series instance UID and the segmentation contour. The contour information is stored in a Contour Image Sequence whose length is determined by however many slices the segmentation is. This sequence must be looped through to find each slice ID (found with the ReferencedSOPInstanceUID tag). get_slice_num(): The instance name of the slice being referenced usually is in the form of \"subseries_num-slice_num.dcm\" (e.g. 1-001.dcm). At this stage, only the slice that was referenced in an annotation will be passed into this function. To get the slice number, the instance name first gets split by \".\" to remove the extention and then gets split by \"-\" to remove the subseries. The nifti files are ordered backwards compared to the DICOM slices and they differ by having a starting index of 0 instead of 1. To make sure the correct nifti slice is selected for the measurement and segmentation, the found slice number gets subtracted from the total number of slices to get the position of the desired nifti slice. Then we add 1 to offset the difference in index.","title":"Developer Notes - Kaitlyn"},{"location":"devnotes/devnotes_kaitlyn/#developer-notes-kaitlyn","text":"","title":"Developer Notes - Kaitlyn"},{"location":"devnotes/devnotes_kaitlyn/#annotation-imaging-segmentation-matching","text":"","title":"Annotation-Imaging-Segmentation Matching"},{"location":"devnotes/devnotes_kaitlyn/#additional-function-documentation-last-updated-2025-09-17","text":"","title":"Additional Function Documentation (Last Updated [2025-09-17])"},{"location":"devnotes/devnotes_kaitlyn/#get_ann_measurements","text":"The DICOM data extracted through pydicom.dcmread is in a similar structure to nested dictionaries. Basic information (PatientID, annotation ID, etc.) can be found from the outer dictionary. The reference imaging information is found within the Current Requested Procedure Evidence Sequence one level down from the outer dictionary. The annotation information is found within nested content sequences, with the outermost content sequence (I've called the parent content sequence) usually containing 5 items with information about language, country of origin, etc., the file itself, the procedure(s) used, and measurements. Measurements are usually the last item in the parent sequence (at index 4). Each measurement is kept in another content sequence. The number of these content sequences are determined by the number of measurements taken. Must loop through all of them to get all annotation measurements. The content sequence each measurement is found in usually has a length of 5. The long axis measurement information is found in the 3rd item (index 2) and the short axis measurement information is found in the 4th item (index 3). Once inside either the long axis or short axis content sequence, you can get the following corresponding information: * Measurement Type: Found by searching the Concept Name Code Sequence for the first item and then obtaining the code meaning. The value of the measurement type should be \"Long Axis\" or \"Short Axis\" depending on what is being looked for. * Measurement Unit: Founed by searching the Measured Value Sequence for the first item, then searching for the Measurement Units Code Sequence first item, and then accessing the Code Value. This is usually \"mm\". * Annotation Measurement: Found by searching the Measured Value Sequence for the first item (same as above) and accessing the Numeric Value * Referenced SOPUID: Denotes the slice that the measurement was taken on (paired long and short axis measurements should be on the same slice). Found by entering the first item of two additional content sequences and the Referenced SOP Sequence and then accessing the Referenced SOP Instance UID value. This number should correspond to an instance value in the matching image (listed for each image in the crawl_db.json file). * Axis Coordinates: Stored as x1/y1/x2/y2 in the DICOM data, these are the points that define the annotation drawn. Found by searching the first item of one additional content sequence and looping through the values of Graphic Data, which should always have a length of 4.","title":"get_ann_measurements()"},{"location":"devnotes/devnotes_kaitlyn/#get_rtstruct_sopuids","text":"This function is currently necessary due to an issue with med-imagetools not extracting the slice IDs associated with RTSTRUCT segmentation files specifially. This type of file will not have any ReferencedSOPUIDs listed in the crawl_db.json function, unlike the SEG files. These SOPUIDs are necessary to match the annotation with the segmentation. Similarly to get_ann_measurements() , the referenced slice IDs are found in a nested dictionary of sequences, each with only one item in them. The Referenced Frame of Reference Sequence contains information related to the slices in the segmentation file, the frame of reference, and the image the segmentation pertains to. The list of slices may only contain slices with actual segmentation or it could contain all slices within the referenced image (including where the binary mask is all 0 for a slice) . Once inside the Referenced Frame of Reference Sequence, the RT Referenced Study Sequence of length 1 will need to be entered. This includes all information in the previous sequence except for the frame of reference information. At this level, you can access the study instance UID and the RT Referenced Series Sequence with length 1. This contains the information related to the imaging series instance UID and the segmentation contour. The contour information is stored in a Contour Image Sequence whose length is determined by however many slices the segmentation is. This sequence must be looped through to find each slice ID (found with the ReferencedSOPInstanceUID tag).","title":"get_rtstruct_SOPUIDs():"},{"location":"devnotes/devnotes_kaitlyn/#get_slice_num","text":"The instance name of the slice being referenced usually is in the form of \"subseries_num-slice_num.dcm\" (e.g. 1-001.dcm). At this stage, only the slice that was referenced in an annotation will be passed into this function. To get the slice number, the instance name first gets split by \".\" to remove the extention and then gets split by \"-\" to remove the subseries. The nifti files are ordered backwards compared to the DICOM slices and they differ by having a starting index of 0 instead of 1. To make sure the correct nifti slice is selected for the measurement and segmentation, the found slice number gets subtracted from the total number of slices to get the position of the desired nifti slice. Then we add 1 to offset the difference in index.","title":"get_slice_num():"},{"location":"devnotes/devnotes_katy/","text":"Developer Notes - Katy SR Exploration [2025-06-04] SR's can be read by MIT index, but it looks like at least the CCC HNSCC ones don't have a ReferenceSeriesUID From Caryn's sr_testing.py script, there is a ReferencedSOPInstanceUID, which means we can determine the slice, just gotta figure out in what CT [2025-06-11] All the SRs we're using are Enhanced Structured Reports Trying to use imgtools SR reader to extract metadata Data Notes CPTAC-HNSCC Annotation files 15 Unique annotation ReferenceSeriesUIDs 10 Images with SeriesUIDs referenced by annotation 8 Masks with the same ReferenceSeriesuUIDs 4 Images with masks and annotations 4 Images with at least one mask, no annotation 97 Images with at least one annotation, no mask 8","title":"Katy"},{"location":"devnotes/devnotes_katy/#developer-notes-katy","text":"","title":"Developer Notes - Katy"},{"location":"devnotes/devnotes_katy/#sr-exploration","text":"","title":"SR Exploration"},{"location":"devnotes/devnotes_katy/#2025-06-04","text":"SR's can be read by MIT index, but it looks like at least the CCC HNSCC ones don't have a ReferenceSeriesUID From Caryn's sr_testing.py script, there is a ReferencedSOPInstanceUID, which means we can determine the slice, just gotta figure out in what CT","title":"[2025-06-04]"},{"location":"devnotes/devnotes_katy/#2025-06-11","text":"All the SRs we're using are Enhanced Structured Reports Trying to use imgtools SR reader to extract metadata","title":"[2025-06-11]"},{"location":"devnotes/devnotes_katy/#data-notes","text":"CPTAC-HNSCC Annotation files 15 Unique annotation ReferenceSeriesUIDs 10 Images with SeriesUIDs referenced by annotation 8 Masks with the same ReferenceSeriesuUIDs 4 Images with masks and annotations 4 Images with at least one mask, no annotation 97 Images with at least one annotation, no mask 8","title":"Data Notes"}]}